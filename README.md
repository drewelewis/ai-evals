# Azure AI Foundry - Agent Evaluation Pipeline

üöÄ **A comprehensive evaluation framework for Azure AI agents using multiple evaluation categories.**

This repository provides a complete evaluation pipeline with **10 specialized evaluators** across 4 categories, designed for thorough assessment of AI agent quality, safety, and performance.

## üìã Overview

Our evaluation framework covers four main categories with **10 active evaluators**:
- **RAG & Retrieval** (3 evaluators) - For information retrieval accuracy: Groundedness, Relevance, Retrieval
- **Agents** (2 evaluators) - For agent-specific behaviors: Intent Resolution, Task Adherence  
- **General Purpose** (3 evaluators) - For overall response quality: Coherence, Fluency, Friendliness (custom)
- **Safety & Security** (2 evaluators) - Content safety and hate/unfairness detection

## ü§ñ How the Evaluations Work

### **üéØ What Gets Evaluated**
This system evaluates **pre-generated AI responses** (not live AI models). You provide a dataset with:
- **Query**: The question asked to the AI
- **Context**: Background information provided to the AI  
- **Response**: The AI's answer (**what gets evaluated**)
- **Ground Truth**: The correct/expected answer (for comparison)

### **‚òÅÔ∏è Execution Environment**
- **Location**: Evaluations run in **Azure AI Foundry** (cloud-based)
- **Process**: Your data is uploaded to Azure, evaluated in the cloud, and results returned
- **Scale**: Can handle datasets from 7 samples (testing) to 50,000+ samples (production)

### **üìä Evaluation Process**
1. **Data Upload**: Your JSONL file is sent to Azure AI Foundry
2. **Batch Processing**: All 7 evaluators analyze each response simultaneously  
3. **Scoring**: Each evaluator gives a 1-5 score plus pass/fail result
4. **Aggregation**: Results combined into a single report with metrics and studio URL

### **üîç What Each Evaluator Does**

#### **Quality Evaluators**
- **Coherence**: Is the response logically organized and sensible?
- **Fluency**: Grammar, clarity, and readability assessment
- **Relevance**: Does the answer directly address the question?

#### **Accuracy Evaluators** 
- **Groundedness**: Is the response based on the provided context?
- **Retrieval**: How well does the response use relevant information?

#### **Agent Evaluators**
- **Intent Resolution**: Does the AI understand what the user really wanted?
- **Task Adherence**: Did the AI follow instructions and meet requirements?

#### **Safety Evaluators**
- **Content Safety**: Detects harmful, inappropriate, or unsafe content
- **Hate/Unfairness**: Identifies biased, discriminatory, or hateful responses

### **üìà Sample Results**
```
Evaluator Results (7 test cases):
‚úÖ fluency: 100% (7/7 passed)
‚úÖ retrieval: 100% (7/7 passed) 
‚ö†Ô∏è coherence: 71% (5/7 passed)
‚ö†Ô∏è relevance: 71% (5/7 passed)
```

### **üèÜ Industry Best Practices**
This evaluation approach mirrors what major AI companies use:
- **Multi-dimensional assessment** (no single metric tells the whole story)
- **Automated scale** (evaluate thousands of responses quickly)
- **Human-AI alignment** (ground truth provides human benchmark)
- **Safety-first approach** (detect harmful content before deployment)

### **üí° Use Cases**
- **Model Comparison**: Test different AI models against same questions
- **Quality Assurance**: Validate AI responses before production deployment
- **Performance Monitoring**: Track AI quality over time
- **Safety Validation**: Ensure responses meet safety standards

## üîß Dependencies & Requirements

### Azure Services Required

This evaluation framework requires several Azure services to function properly:

#### **Azure OpenAI Service** ‚≠ê (Required)
- **Purpose**: Powers all LLM-based evaluators (Coherence, Fluency, Relevance, etc.)
- **Required Model**: GPT-4o (recommended) or GPT-4/GPT-4-turbo
- **Configuration**: 
  - `AZURE_OPENAI_ENDPOINT`: Your Azure OpenAI resource endpoint
  - `AZURE_OPENAI_API_KEY`: Access key for the resource
  - `AZURE_OPENAI_DEPLOYMENT`: Deployment name (e.g., "gpt-4o")
- **Usage**: Core dependency for all evaluation operations

#### **Azure AI Foundry (AI Studio)** ‚≠ê (Required for Cloud Features)
- **Purpose**: Provides specialized evaluators and cloud deployment capabilities
- **Services Used**:
  - **Content Safety API**: For safety evaluators (Hate/Unfairness, Content Safety)
  - **Agent Evaluators**: Intent Resolution, Task Adherence
  - **Evaluation Pipeline**: Cloud deployment and result tracking
- **Configuration**:
  - `AZURE_SUBSCRIPTION_ID`: Your Azure subscription
  - `AZURE_RESOURCE_GROUP`: Resource group containing AI Foundry project
  - `AZURE_AI_FOUNDRY_PROJECT`: AI Foundry project name
  - `PROJECT_ENDPOINT`: AI Foundry project endpoint URL

#### **Azure Machine Learning** (Required for Cloud Deployment)
- **Purpose**: Backend service for Azure AI Foundry evaluations
- **Usage**: Automatic integration through AI Foundry project
- **Authentication**: Uses same credentials as AI Foundry

### Python Dependencies

#### **Core Evaluation Libraries**
```python
azure-ai-evaluation      # Main evaluation framework
azure-identity           # Azure authentication
azure-ai-projects        # AI Foundry project integration
azure-ai-ml             # Machine Learning client (cloud deployment)
```

#### **Model Integration**
```python
openai                  # OpenAI API client for Azure OpenAI
promptflow-core         # Custom evaluator framework
promptflow-azure        # Azure integration for Promptflow
```

#### **Data Processing & Analysis**
```python
pandas                  # Data manipulation for results analysis
numpy                   # Numerical operations
json                    # JSON data handling
```

#### **Environment & Configuration**
```python
python-dotenv           # Environment variable management
typing-extensions       # Enhanced type hints
```

#### **Jupyter Notebook Support**
```python
notebook                # Jupyter notebook server
ipykernel              # Jupyter kernel
IPython                # Interactive Python
matplotlib             # Plotting and visualization
seaborn                # Statistical data visualization
```

### Authentication Requirements

#### **Azure CLI** (Required for Cloud Deployment)
```bash
# Install Azure CLI
# https://docs.microsoft.com/en-us/cli/azure/install-azure-cli

# Login to Azure
az login

# Verify authentication
az account show
```

#### **Service Authentication Methods**
1. **Azure CLI Credentials** (Recommended for development)
   - Uses `az login` session
   - Automatic credential discovery
   - Works with both local and cloud scripts

2. **Service Principal** (Recommended for production)
   - Environment variables for automated deployment
   - `AZURE_CLIENT_ID`, `AZURE_CLIENT_SECRET`, `AZURE_TENANT_ID`

3. **Managed Identity** (For Azure-hosted applications)
   - Automatic authentication when running on Azure
   - No credentials needed in code

### System Requirements

#### **Operating System**
- **Windows**: Windows 10/11 (primary support)
- **Linux**: Ubuntu 18.04+ / RHEL 8+ (community support)
- **macOS**: macOS 10.15+ (community support)

#### **Python Version**
- **Python 3.8+** (Required)
- **Python 3.10+** (Recommended for best compatibility)

#### **Hardware Requirements**
- **Memory**: 4GB RAM minimum, 8GB+ recommended
- **Storage**: 2GB free space for dependencies and results
- **Network**: Stable internet connection for Azure API calls

### Cost Considerations

#### **Azure OpenAI**
- **GPT-4o**: ~$0.005-0.015 per 1K tokens (input/output)
- **Evaluation Volume**: ~7 test cases √ó 10 evaluators = 70 API calls per run
- **Estimated Cost**: $0.10-0.50 per full evaluation run

#### **Azure AI Foundry**
- **Content Safety**: ~$0.001 per text transaction
- **Agent Evaluators**: Included in AI Foundry project quotas
- **Estimated Cost**: $0.01-0.05 per evaluation run

#### **Total Monthly Cost Estimate**
- **Development Usage**: $10-50/month (daily testing)
- **Production Usage**: $100-500/month (continuous monitoring)

> **üí° Cost Optimization Tips**: 
> - Use `run_evaluation_local.py` for development to minimize API calls
> - Reduce sample data size during testing
> - Use GPT-3.5-turbo for cost-sensitive scenarios (lower quality)

### Network & Security Requirements

#### **Outbound Connectivity Required**
```
Azure OpenAI:          *.openai.azure.com:443
Azure AI Foundry:      *.cognitiveservices.azure.com:443
Azure Management:      management.azure.com:443
Azure Storage:         *.blob.core.windows.net:443
```

#### **Data Privacy**
- All evaluation data is processed through your Azure tenant
- No data leaves your Azure subscription boundary
- Evaluation results stored in your AI Foundry project
- Local results saved to `output/` folder

#### **Compliance Features**
- **GDPR Compliant**: Data processing within Azure tenant
- **SOC 2 Type 2**: Azure service compliance
- **HIPAA**: Available with appropriate Azure configuration
- **Data Residency**: Configurable through Azure region selection

## üöÄ Quick Start

### 1. Environment Setup
```bash
# Create virtual environment
_env_create.bat

# Activate virtual environment (Windows)
_env_activate.bat
# OR for PowerShell: .venv\Scripts\Activate.ps1

# Install dependencies
_install.bat
```

### 2. Configuration
```bash
# Copy sample configuration
copy sample.env .env

# Edit .env file with your Azure credentials:
# - AZURE_OPENAI_ENDPOINT
# - AZURE_OPENAI_API_KEY
# - AZURE_OPENAI_DEPLOYMENT (recommended: gpt-4o)
# - Azure AI Foundry project details
```

### 3. Run Evaluations
```bash
# Local evaluation (recommended for development)
python run_evaluation_local.py

# Cloud deployment (for production)
python run_evaluation_with_cloud_upload.py

# View results
jupyter notebook foundry_results_viewer.ipynb
```

## ÔøΩ Project Structure

```
ai-evals/
‚îú‚îÄ‚îÄ run_evaluation_local.py          # Main local evaluation script
‚îú‚îÄ‚îÄ run_evaluation_with_cloud_upload.py  # Cloud deployment script
‚îú‚îÄ‚îÄ model_endpoint.py                # Live agent integration endpoint
‚îú‚îÄ‚îÄ foundry_results_viewer.ipynb     # Results visualization notebook
‚îú‚îÄ‚îÄ requirements.txt                 # Python dependencies
‚îú‚îÄ‚îÄ sample.env                       # Environment configuration template
‚îú‚îÄ‚îÄ .env                            # Your configuration (create from sample.env)
‚îú‚îÄ‚îÄ _env_create.bat                 # Virtual environment setup
‚îú‚îÄ‚îÄ _env_activate.bat               # Environment activation
‚îú‚îÄ‚îÄ _install.bat                    # Dependency installation
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ sample_data.jsonl          # Input test cases (7 scenarios)
‚îú‚îÄ‚îÄ output/                         # Output folder (timestamped results)
‚îÇ   ‚îú‚îÄ‚îÄ foundry_rag_retrieval_results_*.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ foundry_agents_results_*.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ foundry_general_purpose_results_*.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ foundry_safety_security_results_*.jsonl
‚îÇ   ‚îî‚îÄ‚îÄ cloud_results_quality_only.json  # Cloud deployment results
‚îú‚îÄ‚îÄ friendliness/                   # Custom evaluator
‚îÇ   ‚îú‚îÄ‚îÄ friendliness.py            # Friendliness evaluator implementation
‚îÇ   ‚îî‚îÄ‚îÄ friendliness.prompty       # Prompty template
‚îî‚îÄ‚îÄ images/                        # Documentation screenshots
```

## üíª Usage Guide

### Input Data Format

The evaluation system reads from `data/sample_data.jsonl`. Each line contains:

```json
{
  "query": "User question or prompt",
  "context": "Reference documentation or background info",
  "response": "AI agent response to evaluate",
  "ground_truth": "Expected correct answer"
}
```

**Current Sample Data (7 test cases):**
- Items 1-5: Normal architecture questions and responses
- Item 6: Moderately problematic content (for safety testing)
- Item 7: Explicitly unsafe content (hate speech detection)

### Output Structure

Results are saved to `output/` with timestamps:

```
foundry_rag_retrieval_results_20250923_143022.jsonl
foundry_agents_results_20250923_143022.jsonl
foundry_general_purpose_results_20250923_143022.jsonl
foundry_safety_security_results_20250923_143022.jsonl
```

Each JSONL file contains:
```json
{
  "inputs": {...},
  "outputs": {
    "evaluator_name": {"score": 4.0, "reason": "..."},
    "another_evaluator": {"score": 3.5, "reason": "..."}
  }
}
```

### Running Different Evaluation Modes

#### 1. Local Development (Recommended)
```bash
python run_evaluation_local.py
```
**Features:**
- ‚úÖ Runs all 10 evaluators locally
- ‚úÖ Fast iteration for development
- ‚úÖ Detailed console output with progress
- ‚úÖ Saves results to timestamped JSONL files
- ‚úÖ No cloud dependencies beyond Azure OpenAI

**Use Cases:**
- Development and testing
- Quick evaluation iterations
- Debugging evaluator behavior
- Offline evaluation

#### 2. Cloud Deployment (Production)
```bash
python run_evaluation_with_cloud_upload.py
```
**Features:**
- ‚úÖ Deploys evaluators to Azure AI Foundry
- ‚úÖ Uploads results to cloud project
- ‚úÖ Enterprise-grade logging and tracking
- ‚úÖ Supports distributed evaluation
- ‚úÖ Integrates with Azure ML workflows

**Use Cases:**
- Production environments
- Team collaboration
- Compliance and audit trails
- Integration with Azure ML pipelines

**Prerequisites:**
- Azure CLI logged in (`az login`)
- Proper Azure permissions for AI Foundry
- All environment variables configured

### Viewing Results

#### 1. Jupyter Notebook Viewer
```bash
jupyter notebook foundry_results_viewer.ipynb
```
**Features:**
- ‚úÖ Automatically finds latest results
- ‚úÖ Loads all 4 categories into pandas DataFrames
- ‚úÖ Clean visualization of scores and reasons
- ‚úÖ Easy data exploration and analysis

#### 2. Direct File Analysis
Results are in standard JSONL format - read with any JSON tool:
```python
import json
import pandas as pd

# Load results
with open('output/foundry_rag_retrieval_results_latest.jsonl', 'r') as f:
    results = [json.loads(line) for line in f]

# Convert to DataFrame
df = pd.json_normalize(results)
```

### Customizing Evaluations

#### 1. Adding Your Own Test Data
Replace or extend `data/sample_data.jsonl`:
```json
{"query": "Your question", "context": "Your context", "response": "Your response", "ground_truth": "Expected answer"}
{"query": "Another question", "context": "More context", "response": "Another response", "ground_truth": "Another answer"}
```

#### 2. Connecting Live Agents
Modify `model_endpoint.py` to connect to your live agent:
```python
def __call__(self, query: str) -> Response:
    # Replace with your agent endpoint
    response = your_agent.generate(query)
    return {"query": query, "response": response}
```

#### 3. Custom Evaluators
Follow the pattern in `friendliness/`:
1. Create evaluator class with `__call__` method
2. Add prompty template if needed
3. Import and add to evaluation scripts

### Environment Configuration

#### Required Environment Variables
```bash
# Azure OpenAI (required for all evaluators)
AZURE_OPENAI_ENDPOINT=https://your-openai.openai.azure.com/
AZURE_OPENAI_API_KEY=your-api-key
AZURE_OPENAI_DEPLOYMENT=gpt-4o
AZURE_OPENAI_API_VERSION=2024-12-01-preview

# Azure AI Foundry (required for cloud deployment)
AZURE_SUBSCRIPTION_ID=your-subscription-id
AZURE_RESOURCE_GROUP=your-resource-group
AZURE_AI_FOUNDRY_PROJECT=your-project-name
PROJECT_ENDPOINT=https://your-project.services.ai.azure.com/api/projects/your-project
```

#### Optional Configuration
```bash
# Custom model settings
AZURE_OPENAI_DEPLOYMENT=gpt-4o  # Recommended model
# Alternative models: gpt-4, gpt-4-turbo, gpt-35-turbo
```

### Troubleshooting

#### Common Issues

**1. Authentication Errors**
```bash
# Ensure Azure CLI is logged in
az login
az account show

# Verify environment variables
echo $AZURE_OPENAI_ENDPOINT  # (PowerShell: $env:AZURE_OPENAI_ENDPOINT)
```

**2. Missing Dependencies**
```bash
# Reinstall requirements
pip install -r requirements.txt

# Check for conflicts
pip list | findstr azure
```

**3. Evaluation Failures**
- Check console output for specific evaluator errors
- Verify input data format in `sample_data.jsonl`
- Ensure all required fields (query, context, response, ground_truth) are present

**4. No Results Files**
- Check `output/` folder exists
- Verify write permissions
- Look for error messages in console output

**5. 403 Forbidden Errors in Azure AI Studio** ‚ö†Ô∏è
> **Common Issue**: If evaluations worked yesterday but fail today with 403 errors, this is often due to overnight **network access policies** being disabled in managed learning environments.

**Symptoms:**
```
Error: Request failed with status code 403
Trace ID: xxxxx-xxxx-xxxx-xxxx
```

**Solution:**
```bash
# Re-enable public network access on storage account
az storage account update \
  --name staievalshub249798735043 \
  --resource-group ai-evals-rg \
  --public-network-access Enabled

# Verify it's enabled
az storage account show \
  --name staievalshub249798735043 \
  --resource-group ai-evals-rg \
  --query "publicNetworkAccess"
```

**Why This Happens:**
- **Managed learning environments** often disable public network access overnight for security compliance
- **Azure policies** may automatically restrict network access during off-hours
- **Security governance** tools can override network settings periodically

**Prevention:**
- **Document this behavior** for your team
- **Set up monitoring** to alert when network access is disabled
- **Consider using private endpoints** if available in your environment

#### Performance Tips

**1. Faster Development Iterations**
- Use `run_evaluation_local.py` for development
- Reduce sample data size for quick testing
- Focus on specific evaluator categories

**2. Production Optimization**
- Use `run_evaluation_with_cloud_upload.py` for scale
- Batch process large datasets
- Monitor Azure quota usage

**3. Cost Management**
- Use gpt-4o for best results, gpt-35-turbo for cost savings
- Monitor API usage in Azure portal
- Optimize input data length

## üöÄ Main Evaluation Scripts

### `run_evaluation_local.py` - Local Development Script

The **Local Evaluation Script** is the primary tool for development and testing:
**Features:**
- ‚úÖ **Runs 10 active evaluators locally** across 4 categories
- ‚úÖ **Reads from `data/sample_data.jsonl`** automatically
- ‚úÖ **Generates 4 separate JSONL files** for each category
- ‚úÖ **Provides detailed console output** with scores and results
- ‚úÖ **Handles errors gracefully** with proper exception handling
- ‚úÖ **Fast iteration** for development and testing

**Usage:**
```bash
python run_evaluation_local.py
```

**Output Files:**
Results are saved to `output/` with timestamp:
- `foundry_rag_retrieval_results_[timestamp].jsonl`
- `foundry_agents_results_[timestamp].jsonl`
- `foundry_general_purpose_results_[timestamp].jsonl`
- `foundry_safety_security_results_[timestamp].jsonl`

### `run_evaluation_with_cloud_upload.py` - Cloud Deployment Script

The **Cloud Evaluation Script** deploys evaluators to Azure AI Foundry for enterprise-grade evaluation:

**Key Features:**
- ‚úÖ **9 core evaluators** with explicit Azure ML hub connection
- ‚úÖ **Identity-based authentication** using Azure CLI credentials
- ‚úÖ **Detailed status reporting** shows which evaluators succeed/fail
- ‚úÖ **Cloud deployment** uploads results to Azure AI Foundry project
- ‚úÖ **Enterprise integration** with Azure ML workflows
- ‚úÖ **Production-ready** with enhanced error handling

**Evaluator Groups:**
1. **Core Evaluators** (most reliable): Groundedness, Relevance, Coherence, Fluency
2. **Advanced Evaluators**: Retrieval, Intent Resolution, Task Adherence  
3. **Safety Evaluators**: Hate/Unfairness, Content Safety

**Usage:**
```bash
python run_evaluation_with_cloud_upload.py
```

**Output:**
- **Cloud**: Results uploaded to your Azure AI Foundry project
- **Local**: Results saved to `output/` folder
- **Console**: Detailed status of which evaluators loaded successfully
- **Integration**: Available in Azure ML studio for further analysis

**Prerequisites:**
- Azure CLI logged in (`az login`)
- Azure AI Foundry project configured
- Proper Azure permissions for cloud deployment

## üìö Azure AI Foundry Built-in Evaluators Reference

Azure AI Foundry provides a comprehensive suite of **built-in evaluators** across multiple categories. This section documents all available evaluators, including those currently used in this project and additional options for future expansion.

### Evaluator Categories Overview

| Category | Evaluators in This Project | Additional Available Evaluators |
|----------|---------------------------|----------------------------------|
| **General Purpose** | Coherence, Fluency | - |
| **Textual Similarity** | - | Similarity, F1Score, RougeScore, GleuScore, BleuScore, MeteorScore |
| **RAG & Retrieval** | Groundedness, Relevance, Retrieval | GroundednessProEvaluator, DocumentRetrievalEvaluator, ResponseCompletenessEvaluator |
| **Agentic** | IntentResolution, TaskAdherence | - |
| **Risk & Safety** | HateUnfairness, ContentSafety (composite) | Violence, Sexual, SelfHarm, IndirectAttack, ProtectedMaterial, UngroundedAttributes, CodeVulnerability |
| **Composite** | QAEvaluator (not active), ContentSafetyEvaluator | - |
| **Azure OpenAI Graders** | - | AzureOpenAILabelGrader, AzureOpenAIStringCheckGrader, AzureOpenAITextSimilarityGrader, AzureOpenAIGrader |

---

## üîç RAG & Retrieval Evaluators

These evaluators assess how well the system retrieves and uses information from knowledge bases.

### Currently Used in This Project

#### Retrieval ‚úÖ
- **Purpose**: Measures textual quality and relevance of retrieved context chunks for addressing the query (LLM-based, no ground truth required)
- **Implementation**: `RetrievalEvaluator` in `run_evaluation_local.py`
- **Scoring**: 1-5 scale with threshold=3
- **Key Features**: 
  - Evaluates how relevant context chunks are to the query
  - Assesses if most relevant chunks are surfaced at the top
  - No ground truth required (unlike DocumentRetrievalEvaluator)
  - Uses LLM for quality assessment vs. classical IR metrics
- **Use Case**: Evaluating RAG retrieval component effectiveness

#### Groundedness ‚úÖ
- **Purpose**: Measures how consistent the response is with respect to the retrieved context
- **Implementation**: `GroundednessEvaluator` in `run_evaluation_local.py`
- **Scoring**: 1-5 scale (higher = better grounded in context)
- **Example**: Ensures architecture recommendations are based on provided reference material
- **Input Requirements**: `query`, `response`, `context`

#### Relevance ‚úÖ
- **Purpose**: Measures how relevant the response is with respect to the query
- **Implementation**: `RelevanceEvaluator` in `run_evaluation_local.py`
- **Scoring**: 1-5 scale (higher = more relevant)
- **Example**: Ensures responses directly address the architecture questions asked
- **Input Requirements**: `query`, `response`

### Additional Available RAG Evaluators

#### GroundednessProEvaluator
- **Purpose**: Advanced groundedness evaluation powered by Azure AI Content Safety
- **Scoring**: 1-5 scale with enhanced accuracy
- **Key Features**:
  - Uses Azure AI Content Safety backend for more precise evaluation
  - Better detection of subtle hallucinations
  - Requires `azure_ai_project` configuration instead of `model_config`
- **Status**: Preview feature
- **Input Requirements**: `query`, `response`, `context`

#### DocumentRetrievalEvaluator
- **Purpose**: Evaluates retrieval quality using classical information retrieval metrics
- **Scoring**: Precision-based metrics
- **Key Features**:
  - Requires ground truth for comparison
  - Uses traditional IR metrics (not LLM-based)
  - Good for benchmark comparisons
- **Input Requirements**: `query`, `context`, `ground_truth`

#### ResponseCompletenessEvaluator
- **Purpose**: Measures how completely the response addresses all aspects of the query
- **Scoring**: 1-5 scale (higher = more complete)
- **Key Features**:
  - Checks if response covers all necessary points
  - Requires ground truth for comparison
  - Good for ensuring comprehensive answers
- **Input Requirements**: `query`, `response`, `ground_truth`

---

## ü§ñ Agentic Evaluators

These evaluators assess agent-specific capabilities and behaviors, specifically designed for evaluating AI agents and their workflows.

### Currently Used in This Project

#### Intent Resolution ‚úÖ
- **Purpose**: Measures how accurately the agent identifies and addresses user intentions
- **Implementation**: `IntentResolutionEvaluator` in `run_evaluation_local.py`
- **Scoring**: 1-5 scale (higher = better intent understanding)
- **Key Features**:
  - Evaluates if agent correctly interprets user goals
  - Assesses understanding of implicit and explicit intents
  - Supports agent message schema format
- **Example**: Evaluates if the agent correctly understands what the user wants to achieve
- **Input Requirements**: Agent messages or conversation format
- **Note**: Supports Foundry Agent Service message format

#### Task Adherence ‚úÖ
- **Purpose**: Measures how well the agent follows through on identified tasks
- **Implementation**: `TaskAdherenceEvaluator` in `run_evaluation_local.py`
- **Scoring**: 1-5 scale (higher = better task completion)
- **Key Features**:
  - Checks if agent completes assigned tasks
  - Evaluates adherence to system message instructions
  - Assesses consistency across multi-step workflows
- **Example**: Checks if the agent provides actionable solutions for architecture problems
- **Input Requirements**: Agent messages or conversation format
- **Note**: Supports Foundry Agent Service message format

### Additional Available Agentic Evaluators

#### ToolCallAccuracyEvaluator
- **Purpose**: Measures whether the agent made correct function tool calls in response to user requests
- **Scoring**: 1-5 scale (higher = better tool selection)
- **Key Features**:
  - Evaluates if agent selects appropriate tools
  - Checks correct tool parameters
  - Assesses tool call sequencing
- **Use Case**: Evaluating agents that use external tools/functions
- **Input Requirements**: Agent messages with tool call information
- **Note**: Requires agent message schema format

---

## üéØ General Purpose Evaluators

These evaluators assess overall response quality and language characteristics, applicable to most AI applications.

### Currently Used in This Project

#### Coherence ‚úÖ
- **Purpose**: Measures logical consistency and flow of responses
- **Implementation**: `CoherenceEvaluator` in `run_evaluation_local.py`
- **Scoring**: 1-5 scale (higher = more coherent)
- **Key Features**:
  - Evaluates logical structure and organization
  - Checks for consistent argumentation
  - Assesses overall response structure
  - Supports conversation mode (multi-turn)
- **Example**: Ensures architecture recommendations follow logical reasoning
- **Input Requirements**: `query`, `response`

#### Fluency ‚úÖ
- **Purpose**: Measures natural language quality and readability
- **Implementation**: `FluencyEvaluator` in `run_evaluation_local.py`
- **Scoring**: 1-5 scale (higher = more fluent)
- **Key Features**:
  - Evaluates grammar and syntax
  - Assesses readability and clarity
  - Checks natural language flow
  - Supports conversation mode (multi-turn)
- **Example**: Ensures architecture advice is clearly written and grammatically correct
- **Input Requirements**: `query`, `response`

#### Friendliness ‚úÖ (Custom Evaluator)
- **Purpose**: Measures conversational tone and helpfulness
- **Implementation**: `FriendlinessEvaluator` (custom evaluator) in `friendliness/friendliness.py`
- **Scoring**: 1-5 scale (higher = more friendly and helpful)
- **Key Features**:
  - Custom-built evaluator using Prompty template
  - Evaluates tone and approachability
  - Assesses helpfulness and professionalism
- **Example**: Ensures responses maintain a professional and helpful tone
- **Template**: Uses `friendliness.prompty` for structured evaluation

### Composite General Purpose Evaluators

#### QAEvaluator (Question-Answering Composite)
- **Purpose**: Combines multiple quality evaluators for comprehensive Q&A assessment
- **Included Evaluators**: Groundedness, Relevance, Coherence, Fluency, Similarity, F1Score
- **Scoring**: Provides individual scores for each component
- **Key Features**:
  - Single evaluator call returns multiple metrics
  - Comprehensive quality assessment
  - Ideal for Q&A applications
- **Input Requirements**: `query`, `response`, `context`, `ground_truth`
- **Note**: Requires ground truth for similarity metrics

---

## üìä Textual Similarity Evaluators

These evaluators compare generated responses against ground truth using various similarity metrics. All require ground truth for comparison.

### Available Evaluators (Not Currently Used)

#### SimilarityEvaluator
- **Purpose**: Measures semantic similarity between response and ground truth using embeddings
- **Scoring**: 1-5 scale (higher = more similar)
- **Key Features**:
  - Uses embeddings for semantic comparison
  - AI-assisted evaluation
  - Good for paraphrase detection
- **Input Requirements**: `query`, `response`, `ground_truth`

#### F1ScoreEvaluator
- **Purpose**: Calculates F1 score between response and ground truth
- **Scoring**: 0-1 scale (1 = perfect match)
- **Key Features**:
  - Token-level comparison
  - Balances precision and recall
  - Classical NLP metric
- **Input Requirements**: `response`, `ground_truth`

#### RougeScoreEvaluator
- **Purpose**: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score
- **Scoring**: 0-1 scale for multiple ROUGE variants
- **Key Features**:
  - Measures n-gram overlap
  - Standard for summarization tasks
  - Multiple variants (ROUGE-1, ROUGE-2, ROUGE-L)
- **Input Requirements**: `response`, `ground_truth`

#### GleuScoreEvaluator
- **Purpose**: GLEU (Google-BLEU) score for translation and generation tasks
- **Scoring**: 0-1 scale (higher = better match)
- **Key Features**:
  - Modified BLEU for shorter texts
  - Good for sentence-level evaluation
  - Less sensitive to length variations
- **Input Requirements**: `response`, `ground_truth`

#### BleuScoreEvaluator
- **Purpose**: BLEU (Bilingual Evaluation Understudy) score for machine translation
- **Scoring**: 0-1 scale (higher = better match)
- **Key Features**:
  - Standard metric for translation tasks
  - N-gram precision-based
  - Includes brevity penalty
- **Input Requirements**: `response`, `ground_truth`

#### MeteorScoreEvaluator
- **Purpose**: METEOR (Metric for Evaluation of Translation with Explicit ORdering) score
- **Scoring**: 0-1 scale (higher = better match)
- **Key Features**:
  - Considers synonyms and stemming
  - More flexible than BLEU
  - Better correlation with human judgment
- **Input Requirements**: `response`, `ground_truth`

---

## üõ°Ô∏è Risk & Safety Evaluators

These evaluators detect potentially harmful or unsafe content using Azure AI Content Safety services.

### Currently Used in This Project

#### HateUnfairnessEvaluator ‚úÖ
- **Purpose**: Identifies biased, discriminatory, or hateful content
- **Implementation**: `HateUnfairnessEvaluator` in `run_evaluation_local.py`
- **Scoring**: 0-7 scale (0-2 = safe, 3+ = unsafe)
- **Threshold**: 3 (scores ‚â•3 are flagged as "fail")
- **Key Features**:
  - Detects discriminatory language
  - Identifies bias and stereotypes
  - Supports conversation and image inputs
  - Powered by Azure AI Content Safety
- **Output**: Provides detailed reasoning for scores
- **Input Requirements**: `query`, `response`

#### ContentSafetyEvaluator ‚úÖ (Composite)
- **Purpose**: Comprehensive assessment of various safety concerns in a single evaluator
- **Implementation**: `ContentSafetyEvaluator` in `run_evaluation_local.py`
- **Categories Covered**:
  - **Hate**: Discriminatory or biased content
  - **Sexual**: Inappropriate sexual content
  - **Violence**: Violent content or incitement
  - **Self-Harm**: Content promoting or describing self-harm
- **Scoring**: 0-7 scale per category (0-2 = safe, 3+ = unsafe)
- **Threshold**: 3 for all categories
- **Key Features**:
  - Single evaluator returns multiple safety metrics
  - Powered by Azure AI Content Safety
  - Supports conversation and image inputs
- **Output**: Detailed breakdown of all safety categories with individual scores and reasons
- **Input Requirements**: `query`, `response`

### Additional Available Safety Evaluators

#### ViolenceEvaluator
- **Purpose**: Detects violent content or content that incites violence
- **Scoring**: 0-7 scale (0-2 = safe, 3+ = unsafe)
- **Key Features**:
  - Standalone violence detection
  - Supports text and image inputs
  - More detailed than composite ContentSafetyEvaluator
- **Input Requirements**: `query`, `response`
- **Note**: Also included in ContentSafetyEvaluator composite

#### SexualEvaluator
- **Purpose**: Detects inappropriate sexual content
- **Scoring**: 0-7 scale (0-2 = safe, 3+ = unsafe)
- **Key Features**:
  - Standalone sexual content detection
  - Supports text and image inputs
  - More detailed than composite ContentSafetyEvaluator
- **Input Requirements**: `query`, `response`
- **Note**: Also included in ContentSafetyEvaluator composite

#### SelfHarmEvaluator
- **Purpose**: Detects content promoting or describing self-harm
- **Scoring**: 0-7 scale (0-2 = safe, 3+ = unsafe)
- **Key Features**:
  - Standalone self-harm detection
  - Supports text and image inputs
  - Critical for mental health safety
- **Input Requirements**: `query`, `response`
- **Note**: Also included in ContentSafetyEvaluator composite

#### IndirectAttackEvaluator
- **Purpose**: Detects indirect attacks, jailbreak attempts, and prompt injection
- **Scoring**: Pass/Fail or 0-7 scale
- **Key Features**:
  - Identifies indirect prompt attacks
  - Detects jailbreak attempts
  - Assesses prompt injection vulnerabilities
  - Supports conversation mode
- **Input Requirements**: `conversation` (multi-turn format)
- **Use Case**: Protecting against adversarial attacks

#### ProtectedMaterialEvaluator
- **Purpose**: Detects responses containing protected or copyrighted material
- **Scoring**: Pass/Fail or 0-7 scale
- **Key Features**:
  - Identifies copyrighted content
  - Detects protected material reproduction
  - Supports text and image inputs
- **Input Requirements**: `query`, `response`
- **Use Case**: Copyright and IP compliance

#### UngroundedAttributesEvaluator
- **Purpose**: Detects false or unverified attributions in responses
- **Scoring**: 0-7 scale or Pass/Fail
- **Key Features**:
  - Identifies fabricated facts
  - Detects incorrect attributions
  - Single-turn text only
- **Input Requirements**: `query`, `response`
- **Use Case**: Fact-checking and attribution accuracy

#### CodeVulnerabilityEvaluator
- **Purpose**: Detects security vulnerabilities in generated code
- **Scoring**: Pass/Fail or severity levels
- **Key Features**:
  - Identifies common code vulnerabilities
  - Security best practice validation
  - Single-turn text only
- **Input Requirements**: `query`, `response` (containing code)
- **Use Case**: Secure code generation validation

---

## üéì Azure OpenAI Graders

Flexible graders that allow custom evaluation criteria using Azure OpenAI models. These are advanced tools for creating specialized evaluations.

### Available Graders (Not Currently Used)

#### AzureOpenAILabelGrader
- **Purpose**: Custom label-based grading with predefined categories
- **Scoring**: User-defined labels/categories
- **Key Features**:
  - Define custom evaluation labels
  - Classify responses into categories
  - Supports conversation mode
- **Use Case**: Custom classification tasks
- **Input Requirements**: `conversation`, custom label definitions

#### AzureOpenAIStringCheckGrader
- **Purpose**: Check for presence or absence of specific strings/patterns
- **Scoring**: Pass/Fail based on string matching
- **Key Features**:
  - Pattern matching evaluation
  - Custom string validation
  - Supports conversation mode
- **Use Case**: Format validation, keyword checking
- **Input Requirements**: `conversation`, string patterns

#### AzureOpenAITextSimilarityGrader
- **Purpose**: Custom semantic similarity evaluation
- **Scoring**: User-defined similarity scale
- **Key Features**:
  - Flexible similarity assessment
  - Requires ground truth
  - Supports conversation mode
- **Use Case**: Custom similarity benchmarks
- **Input Requirements**: `conversation`, `ground_truth`

#### AzureOpenAIGrader
- **Purpose**: General-purpose custom grader with flexible criteria
- **Scoring**: User-defined scoring system
- **Key Features**:
  - Fully customizable evaluation criteria
  - Define your own prompts and rubrics
  - Supports conversation mode
- **Use Case**: Any custom evaluation scenario
- **Input Requirements**: `conversation`, custom evaluation prompt

---

## üîß Evaluator Configuration Requirements

### Model Configuration Setup

Different evaluators have different configuration requirements:

#### AI-Assisted Quality Evaluators
**Require `model_config` with GPT model:**
- Coherence, Fluency, Relevance, Groundedness, Retrieval
- Intent Resolution, Task Adherence, Tool Call Accuracy
- Similarity, Response Completeness

**Recommended Models:**
- `gpt-4o` (best performance, recommended)
- `gpt-4o-mini` (cost-effective alternative to gpt-3.5-turbo)
- `gpt-4`, `gpt-4-turbo` (good performance)

**Example Configuration:**
```python
model_config = {
    "azure_endpoint": os.getenv("AZURE_OPENAI_ENDPOINT"),
    "api_key": os.getenv("AZURE_OPENAI_API_KEY"),
    "azure_deployment": os.getenv("AZURE_OPENAI_DEPLOYMENT"),
    "api_version": "2024-12-01-preview"
}
```

#### Safety Evaluators & GroundednessProEvaluator
**Require `azure_ai_project` configuration:**
- All safety evaluators (Violence, Sexual, SelfHarm, HateUnfairness, etc.)
- ContentSafetyEvaluator (composite)
- GroundednessProEvaluator

**Example Configuration:**
```python
azure_ai_project = {
    "subscription_id": os.getenv("AZURE_SUBSCRIPTION_ID"),
    "resource_group_name": os.getenv("AZURE_RESOURCE_GROUP"),
    "project_name": os.getenv("AZURE_AI_FOUNDRY_PROJECT")
}
```

#### NLP Metrics (No Configuration Required)
**Classic NLP evaluators don't need model or project config:**
- F1Score, RougeScore, GleuScore, BleuScore, MeteorScore

### Conversation vs Single-Turn Support

| Support Level | Evaluators |
|--------------|------------|
| **Conversation & Single-Turn (Text)** | Coherence, Fluency, Relevance, Groundedness, GroundednessPro, Retrieval, IntentResolution, TaskAdherence, ToolCallAccuracy, IndirectAttack, Azure OpenAI Graders |
| **Conversation & Single-Turn (Text + Image)** | Violence, Sexual, SelfHarm, HateUnfairness, ProtectedMaterial, ContentSafety |
| **Single-Turn Only (Text)** | Similarity, F1Score, RougeScore, GleuScore, BleuScore, MeteorScore, ResponseCompleteness, DocumentRetrieval, UngroundedAttributes, CodeVulnerability, QAEvaluator |

### Ground Truth Requirements

| Requires Ground Truth | Evaluators |
|-----------------------|------------|
| **Yes** | Similarity, F1Score, RougeScore, GleuScore, BleuScore, MeteorScore, DocumentRetrieval, ResponseCompleteness, QAEvaluator (partial), Azure OpenAI Text Similarity Grader |
| **No** | All other evaluators |

---

## üìä Evaluation Data

### Sample Data (`data/sample_data.jsonl`)
Contains **7 architecture-related scenarios** including:
- ‚úÖ **Normal Cases**: Proper architecture questions and responses (Items 1-5)
- ‚úÖ **Moderately Problematic**: Offensive but not explicitly hateful content (Item 6)
- ‚úÖ **Explicitly Unsafe**: Hateful, discriminatory content for safety testing (Item 7)

### Data Structure
Each item contains:
```json
{
  "query": "User question",
  "context": "Reference documentation",
  "response": "Agent response", 
  "ground_truth": "Expected answer"
}
```

### üîå Live Agent Integration: `model_endpoint.py`

The **Model Endpoint** is a placeholder endpoint designed for integration with live agentic applications.

#### Current Usage (Static Data)
- ‚úÖ Evaluation uses pre-generated responses from `sample_data.jsonl`
- ‚úÖ Responses are stored as static text in the `"response"` field
- ‚úÖ Good for testing and development with known data

#### Future Usage (Live Agent Integration)
- üîÑ **Connect to running agent**: Replace static responses with live agent calls
- üîÑ **Dynamic evaluation**: Pass `query` and `context` to live agent endpoint
- üîÑ **Real-time responses**: Get fresh responses for each evaluation run
- üîÑ **Production testing**: Evaluate actual agent performance in real scenarios


#### Benefits of Live Integration
- **üéØ Real Performance**: Evaluate actual agent behavior, not pre-written responses
- **üîÑ Dynamic Testing**: Each run tests current agent state and capabilities  
- **üìà Continuous Monitoring**: Regular evaluation of deployed agents
- **üöÄ Production Ready**: Bridge between development testing and production monitoring

#### Migration Path
1. **Phase 1**: Use static data for pipeline development ‚úÖ (Current)
2. **Phase 2**: Integrate `model_endpoint.py` with your agentic application üîÑ (Future)
3. **Phase 3**: Switch evaluation pipeline to use live responses üîÑ (Future)
4. **Phase 4**: Set up continuous evaluation monitoring üîÑ (Future)

## üìà Results Viewer: `foundry_results_viewer.ipynb`

Simple Jupyter notebook with **7 cells** that:
- ‚úÖ **Automatically finds latest results** based on timestamp
- ‚úÖ **Loads all 4 categories** into pandas DataFrames
- ‚úÖ **Displays results** with shape information and full data
- ‚úÖ **Minimal and clean** interface for quick result inspection

### Usage
1. Open `foundry_results_viewer.ipynb`
2. Run all cells
3. View DataFrames for each evaluation category

## üéØ Key Insights from Our Testing

### Content Safety Effectiveness
Based on testing with problematic content (Items 6-7):

- ‚úÖ **Intent/Task Evaluators**: Successfully identify problematic content (scores 1.0/5.0)
- ‚úÖ **Hate/Unfairness Evaluator**: Correctly flags explicit hate speech (score 6, result: "fail")
- ‚úÖ **Content Safety Composite**: Provides detailed breakdown:
  - Item 6 (moderate): Violence=3, others=0
  - Item 7 (explicit): Hate=6, Violence=4, others=0
- ‚úÖ **Threshold System**: Scores ‚â•3 properly trigger "fail" results

### Quality Measurement
- ‚úÖ **Multi-Dimensional**: Different evaluators catch different issues
- ‚úÖ **Complementary**: Combining approaches provides comprehensive coverage
- ‚úÖ **Actionable**: Results clearly indicate specific areas for improvement
- ‚úÖ **Scalable**: Single script handles all evaluation categories efficiently

### Agent Behavior Detection
- ‚úÖ **Intent Resolution**: Ranges from 1.0 (problematic content) to 5.0 (clear technical questions)
- ‚úÖ **Task Adherence**: Shows degradation from 5.0 (complete answers) to 1.0 (inappropriate responses)
- ‚úÖ **Correlation**: Agent evaluators correlate well with content safety findings

## üîß Getting Started

### Prerequisites
- Python 3.8+ installed
- Azure subscription with OpenAI and AI Foundry access
- Git (for cloning the repository)

### Step-by-Step Setup

#### 1. Clone and Setup Environment
```bash
# Clone the repository
git clone <repository-url>
cd ai-evals

# Create virtual environment
_env_create.bat

# Activate environment (Windows Command Prompt)
_env_activate.bat

# For PowerShell users:
.venv\Scripts\Activate.ps1

# Install dependencies
_install.bat
```

#### 2. Configure Azure Services
```bash
# Copy environment template
copy sample.env .env

# Edit .env file with your Azure credentials
notepad .env
```

**Required Configuration:**
```bash
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_KEY=your-api-key-here
AZURE_OPENAI_DEPLOYMENT=gpt-4o
AZURE_SUBSCRIPTION_ID=your-subscription-id
AZURE_RESOURCE_GROUP=your-resource-group
AZURE_AI_FOUNDRY_PROJECT=your-project-name
```

#### 3. Verify Setup
```bash
# Test Azure CLI authentication (for cloud deployment)
az login
az account show

# Verify environment variables
python -c "from dotenv import load_dotenv; load_dotenv(); import os; print('OpenAI Endpoint:', os.getenv('AZURE_OPENAI_ENDPOINT'))"
```

#### 4. Run Your First Evaluation
```bash
# Start with local evaluation
python run_evaluation_local.py

# Check results
dir output\

# View results in Jupyter
jupyter notebook foundry_results_viewer.ipynb
```

### Next Steps
1. **Customize Input Data**: Modify `data/sample_data.jsonl` with your test cases
2. **Connect Live Agent**: Update `model_endpoint.py` for dynamic evaluation
3. **Deploy to Cloud**: Use `run_evaluation_with_cloud_upload.py` for production
4. **Automate Workflows**: Set up CI/CD pipelines for continuous evaluation

## üìù Configuration

### Azure Services Required
- **Azure OpenAI**: For LLM-based evaluators (GPT-4o recommended)
- **Azure AI Foundry**: For content safety and agent evaluators
- **Default Azure Credential**: For authentication

### Evaluation Thresholds
- **Safety Evaluators**: threshold=3 (scores ‚â•3 flagged as unsafe)
- **Quality Evaluators**: 1-5 scale (higher = better quality)
- **Agent Evaluators**: 1-5 scale (higher = better performance)

## üèóÔ∏è Architecture

```
Azure AI Foundry Evaluation Pipeline
‚îú‚îÄ‚îÄ Input Layer
‚îÇ   ‚îú‚îÄ‚îÄ data/sample_data.jsonl (test cases)
‚îÇ   ‚îî‚îÄ‚îÄ model_endpoint.py (live agent integration)
‚îú‚îÄ‚îÄ Evaluation Engine
‚îÇ   ‚îú‚îÄ‚îÄ run_evaluation_local.py (development)
‚îÇ   ‚îî‚îÄ‚îÄ run_evaluation_with_cloud_upload.py (production)
‚îú‚îÄ‚îÄ Evaluator Categories
‚îÇ   ‚îú‚îÄ‚îÄ RAG & Retrieval (3): Groundedness, Relevance, Retrieval
‚îÇ   ‚îú‚îÄ‚îÄ Agents (2): Intent Resolution, Task Adherence
‚îÇ   ‚îú‚îÄ‚îÄ General Purpose (3): Coherence, Fluency, Friendliness
‚îÇ   ‚îî‚îÄ‚îÄ Safety & Security (2): Hate/Unfairness, Content Safety
‚îú‚îÄ‚îÄ Output Layer
‚îÇ   ‚îú‚îÄ‚îÄ output/ (timestamped JSONL files)
‚îÇ   ‚îú‚îÄ‚îÄ Azure AI Foundry (cloud storage)
‚îÇ   ‚îî‚îÄ‚îÄ foundry_results_viewer.ipynb (visualization)
‚îî‚îÄ‚îÄ Configuration
    ‚îú‚îÄ‚îÄ .env (Azure credentials)
    ‚îú‚îÄ‚îÄ requirements.txt (dependencies)
    ‚îî‚îÄ‚îÄ setup scripts (_env_*.bat, _install.bat)
```

This evaluation framework provides comprehensive coverage across all major aspects of AI agent quality, safety, and performance in a streamlined, easy-to-use pipeline. 

## üìÅ Sample Results Files (Removed)

**Note:** The sample result files (`cloud_results_full_with_safety.json` and `cloud_results_quality_only.json`) have been removed to keep the project focused on actual evaluation workflows.

**Why removed?**
- ‚úÖ **Cleaner organization**: Clear distinction between input data and output results
- ‚úÖ **Reduced confusion**: No mixing of sample files with actual evaluation outputs
- ‚úÖ **Better workflow**: Users focus on running evaluations rather than studying static examples

**Understanding Output Format:**
- Run `python run_evaluation_local.py` to see actual output format
- Check `output/` folder for timestamped results
- Use `foundry_results_viewer.ipynb` to explore result structure

**Previous Sample Files:**
- `cloud_results_full_with_safety.json` ‚Üí Example with all 11 evaluators (including safety)
- `cloud_results_quality_only.json` ‚Üí Example with 9 quality evaluators only

**Current Workflow:**
1. **Input**: `data/sample_data.jsonl` (your test cases)
2. **Processing**: Run evaluation scripts
3. **Output**: Timestamped JSONL files in `output/`
4. **Analysis**: Use Jupyter notebook for visualization

### Example Outputs on Azure AI Foundry

Below are sample evaluation outputs from running the run_evaluation_with_cloud_upload.py script as seen on Azure AI Foundry:
These screenshots demonstrate the comprehensive evaluation results across all categories, including safety scores, quality metrics, and agent performance measurements.

![Detailed Evaluation Metrics](/images/foundry-outputs-2.jpg)
![Foundry Evaluation Results](/images/foundry-outputs.jpg)

